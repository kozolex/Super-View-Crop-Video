{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "model = YOLO('yolov8n.pt')  # Model lekki, można użyć np. 'yolov8m.pt' dla lepszej dokładności\n",
    "\n",
    "# Ścieżki do plików wejściowego i wyjściowego\n",
    "#input_path = 'F://BurzaVSZryw_mini.mp4'\n",
    "input_path = '/media/512GB_ext/BurzaVSZryw_mini.mp4'\n",
    "output_path = 'test.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # 2704\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 900\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Obliczenie rozmiaru kadru dla proporcji 16:9\n",
    "crop_height = height  # 900\n",
    "crop_width = int(crop_height * 16 / 9)  # 1600\n",
    "\n",
    "# Inicjalizacja zapisu wideo\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "# Parametry wygładzania pozycji kadru\n",
    "window_size = 30  # Liczba klatek do średniej kroczącej\n",
    "positions = []\n",
    "\n",
    "def calculate_center(detections):\n",
    "    \"\"\"Oblicza centrum akcji na podstawie wykrytych obiektów.\"\"\"\n",
    "    if not detections:\n",
    "        return width // 2  # Domyślnie środek, jeśli brak detekcji\n",
    "    centroids = []\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det[:4]\n",
    "        centroids.append((x1 + x2) / 2)  # Pozycja X środka bounding boxa\n",
    "    if centroids:\n",
    "        return sum(centroids) / len(centroids)\n",
    "    return width // 2\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Detekcja obiektów w klatce\n",
    "    results = model(frame)\n",
    "    detections = results[0].boxes.data.cpu().numpy()  # Format: [x1, y1, x2, y2, confidence, class]\n",
    "    \n",
    "    # Filtrowanie detekcji: zawodnicy (class 0 - 'person') i piłka (class 32 - 'sports ball')\n",
    "    players = [det for det in detections if int(det[5]) == 0]\n",
    "    balls = [det for det in detections if int(det[5]) == 32]\n",
    "    \n",
    "    # Obliczenie centrum akcji\n",
    "    if balls:\n",
    "        # Jeśli wykryto piłkę, użyj jej pozycji\n",
    "        center_x = calculate_center(balls)\n",
    "    else:\n",
    "        # W przeciwnym razie użyj centroidu zawodników\n",
    "        center_x = calculate_center(players)\n",
    "    \n",
    "    # Wygładzenie pozycji kadru\n",
    "    positions.append(center_x)\n",
    "    if len(positions) > window_size:\n",
    "        positions.pop(0)\n",
    "    smoothed_center_x = sum(positions) / len(positions)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    right = left + crop_width\n",
    "    \n",
    "    # Przycięcie klatki\n",
    "    cropped_frame = frame[:, left:right]\n",
    "    \n",
    "    # Zapis przyciętej klatki\n",
    "    out.write(cropped_frame)\n",
    "\n",
    "# Zwolnienie zasobów\n",
    "cap.release()\n",
    "out.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m detections \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tile, tile_x, tile_y \u001b[38;5;129;01min\u001b[39;00m tiles:\n\u001b[0;32m---> 80\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     tile_detections \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m tile_detections:\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/engine/model.py:185\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/engine/model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/engine/predictor.py:227\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/engine/predictor.py:330\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 330\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/engine/predictor.py:182\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    178\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:636\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 636\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/tasks.py:179\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    180\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:318\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:318\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:495\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:92\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip, VideoClip\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Wyłączenie szczegółowych logów YOLOv8\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "# Sprawdzenie dostępności GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "model = YOLO('yolov8n.pt').to(device)  # Przeniesienie modelu na GPU, jeśli dostępne\n",
    "\n",
    "# Ścieżki do plików\n",
    "input_path = '/media/512GB_ext/BurzaVSZryw_mini.mp4'\n",
    "output_path = 'test.mp4'\n",
    "temp_video_path = f'temp_output_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Docelowy rozmiar kadru (16:9, wysokość 720)\n",
    "crop_height = 720\n",
    "crop_width = int(crop_height * 16 / 9)  # 1280\n",
    "\n",
    "# Parametry wygładzania\n",
    "window_size = 15\n",
    "positions_x = []\n",
    "positions_y = []\n",
    "smoothing_factor = 0.1\n",
    "\n",
    "# Inicjalizacja zapisu tymczasowego wideo bez dźwięku\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(temp_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "def split_frame(frame, tile_size=640):\n",
    "    \"\"\"Dzieli obraz na kafelki dla lepszej detekcji małych obiektów.\"\"\"\n",
    "    tiles = []\n",
    "    h, w = frame.shape[:2]\n",
    "    for y in range(0, h, tile_size):\n",
    "        for x in range(0, w, tile_size):\n",
    "            tile = frame[y:min(y + tile_size, h), x:min(x + tile_size, w)]\n",
    "            tiles.append((tile, x, y))\n",
    "    return tiles\n",
    "\n",
    "def calculate_center(detections, frame_width, frame_height):\n",
    "    \"\"\"Oblicza centrum akcji, priorytetyzując piłkę.\"\"\"\n",
    "    if not detections:\n",
    "        return frame_width // 2, frame_height // 2\n",
    "    balls = [det for det in detections if int(det[5]) == 32]  # 'sports ball'\n",
    "    players = [det for det in detections if int(det[5]) == 0]  # 'person'\n",
    "    \n",
    "    if balls:\n",
    "        x1, y1, x2, y2 = balls[0][:4]\n",
    "        return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    elif players:\n",
    "        centroids_x = [(det[0] + det[2]) / 2 for det in players]\n",
    "        centroids_y = [(det[1] + det[3]) / 2 for det in players]\n",
    "        return sum(centroids_x) / len(centroids_x), sum(centroids_y) / len(centroids_y)\n",
    "    return frame_width // 2, frame_height // 2\n",
    "\n",
    "prev_center_x, prev_center_y = width // 2, height // 2\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Podział na kafelki dla detekcji\n",
    "    tiles = split_frame(frame, tile_size=640)\n",
    "    detections = []\n",
    "    for tile, tile_x, tile_y in tiles:\n",
    "        results = model(tile, classes=[0, 32], device=device)\n",
    "        tile_detections = results[0].boxes.data.cpu().numpy()\n",
    "        for det in tile_detections:\n",
    "            det[0] += tile_x\n",
    "            det[2] += tile_x\n",
    "            det[1] += tile_y\n",
    "            det[3] += tile_y\n",
    "            detections.append(det)\n",
    "    \n",
    "    # Obliczenie centrum akcji\n",
    "    center_x, center_y = calculate_center(detections, width, height)\n",
    "    \n",
    "    # Wygładzanie liniowe\n",
    "    center_x = prev_center_x * (1 - smoothing_factor) + center_x * smoothing_factor\n",
    "    center_y = prev_center_y * (1 - smoothing_factor) + center_y * smoothing_factor\n",
    "    prev_center_x, prev_center_y = center_x, center_y\n",
    "    \n",
    "    # Wygładzanie średnią kroczącą\n",
    "    positions_x.append(center_x)\n",
    "    positions_y.append(center_y)\n",
    "    if len(positions_x) > window_size:\n",
    "        positions_x.pop(0)\n",
    "        positions_y.pop(0)\n",
    "    smoothed_center_x = sum(positions_x) / len(positions_x)\n",
    "    smoothed_center_y = sum(positions_y) / len(positions_y)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    top = int(max(0, min(smoothed_center_y - crop_height / 2, height - crop_height)))\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    \n",
    "    # Przycięcie klatki\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Skalowanie do 1280x720, jeśli kadr nie jest dokładny\n",
    "    if cropped_frame.shape != (crop_height, crop_width, 3):\n",
    "        cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Zapis klatki do tymczasowego pliku\n",
    "    out.write(cropped_frame)\n",
    "\n",
    "# Zwolnienie zasobów OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dodanie dźwięku za pomocą moviepy\n",
    "print(\"Adding audio to the output video...\")\n",
    "input_clip = VideoFileClip(input_path)\n",
    "temp_clip = VideoFileClip(temp_video_path)\n",
    "\n",
    "# Ustawienie dźwięku z oryginalnego wideo\n",
    "final_clip = temp_clip.set_audio(input_clip.audio)\n",
    "\n",
    "# Zapis finalnego wideo z dźwiękiem\n",
    "final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "# Zamknięcie klipów\n",
    "input_clip.close()\n",
    "temp_clip.close()\n",
    "final_clip.close()\n",
    "\n",
    "# Usunięcie tymczasowego pliku\n",
    "os.remove(temp_video_path)\n",
    "print(\"Output video with audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 959/959 [00:25<00:00, 37.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding audio to the output video...\n",
      "Moviepy - Building video test.mp4.\n",
      "MoviePy - Writing audio in testTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n",
      "Output video with audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Wyłączenie szczegółowych logów YOLOv8\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "# Sprawdzenie dostępności GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "#model = YOLO('yolov8n.pt').to(device)  # Przeniesienie modelu na GPU, jeśli dostępne\n",
    "model = YOLO('yolov9c.pt').to(device)\n",
    "model.set_classes = ([\"player\",\"referee\", \"ball\"])\n",
    "# Ścieżki do plików\n",
    "input_path = '/media/512GB_ext/BurzaVSZryw_mini.mp4'\n",
    "output_path = 'test.mp4'\n",
    "temp_video_path = f'temp_output_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Docelowy rozmiar kadru (16:9, wysokość 720)\n",
    "crop_height = 720\n",
    "crop_width = int(crop_height * 16 / 9)  # 1280\n",
    "\n",
    "# Parametry wygładzania\n",
    "window_size = 15\n",
    "positions_x = []\n",
    "positions_y = []\n",
    "smoothing_factor = 0.1\n",
    "\n",
    "# Parametry dla skupienia na piłce\n",
    "frames_without_ball_threshold = int(fps * 3)  # 3 sekundy\n",
    "frames_without_ball = 0\n",
    "last_ball_position = None\n",
    "\n",
    "# Inicjalizacja zapisu tymczasowego wideo bez dźwięku\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(temp_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "def resize_frame(frame, scale=0.5):\n",
    "    \"\"\"Zmniejsza obraz o podany współczynnik skali.\"\"\"\n",
    "    return cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def scale_detections(detections, scale):\n",
    "    \"\"\"Przeskalowuje współrzędne detekcji do oryginalnych wymiarów.\"\"\"\n",
    "    for det in detections:\n",
    "        det[0] /= scale  # x1\n",
    "        det[2] /= scale  # x2\n",
    "        det[1] /= scale  # y1\n",
    "        det[3] /= scale  # y2\n",
    "    return detections\n",
    "\n",
    "def draw_detections(frame, detections, left, top, right, bottom):\n",
    "    \"\"\"Rysuje bounding boxy i nazwy klas na przyciętym obrazie.\"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        # Sprawdzenie, czy detekcja jest w przyciętym obszarze\n",
    "        if x1 < right and x2 > left and y1 < bottom and y2 > top:\n",
    "            # Przesunięcie współrzędnych do przyciętego obszaru\n",
    "            draw_x1 = int(max(x1 - left, 0))\n",
    "            draw_y1 = int(max(y1 - top, 0))\n",
    "            draw_x2 = int(min(x2 - left, crop_width))\n",
    "            draw_y2 = int(min(y2 - top, crop_height))\n",
    "            label = model.names[int(cls)]\n",
    "            cv2.rectangle(frame, (draw_x1, draw_y1), (draw_x2, draw_y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (draw_x1, draw_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "def calculate_center(detections, frame_width, frame_height, use_ball=True):\n",
    "    \"\"\"Oblicza centrum akcji, priorytetyzując piłkę lub zawodników.\"\"\"\n",
    "    if use_ball:\n",
    "        balls = [det for det in detections if int(det[5]) == 32]  # 'sports ball'\n",
    "        if balls:\n",
    "            x1, y1, x2, y2 = balls[0][:4]\n",
    "            return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    players = [det for det in detections if int(det[5]) == 0]  # 'person'\n",
    "    if players:\n",
    "        centroids_x = [(det[0] + det[2]) / 2 for det in players]\n",
    "        centroids_y = [(det[1] + det[3]) / 2 for det in players]\n",
    "        return sum(centroids_x) / len(centroids_x), sum(centroids_y) / len(centroids_y)\n",
    "    return frame_width // 2, frame_height // 2\n",
    "\n",
    "prev_center_x, prev_center_y = width // 2, height // 2\n",
    "\n",
    "# Pasek postępu\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Zmniejszenie obrazu o połowę dla detekcji\n",
    "    small_frame = resize_frame(frame, scale=0.5)\n",
    "    small_height, small_width = small_frame.shape[:2]\n",
    "    \n",
    "    # Detekcja na pomniejszonym obrazie\n",
    "    results = model(small_frame, classes=[0, 32], device=device)\n",
    "    detections = results[0].boxes.data.cpu().numpy()\n",
    "    \n",
    "    # Przeskalowanie detekcji do oryginalnych wymiarów\n",
    "    detections = scale_detections(detections, scale=0.5)\n",
    "    \n",
    "    # Sprawdzenie, czy wykryto piłkę\n",
    "    balls = [det for det in detections if int(det[5]) == 32]\n",
    "    if balls:\n",
    "        frames_without_ball = 0\n",
    "        last_ball_position = balls[0][:4]\n",
    "    else:\n",
    "        frames_without_ball += 1\n",
    "    \n",
    "    # Decyzja, czy skupić się na piłce czy zawodnikach\n",
    "    if frames_without_ball <= frames_without_ball_threshold and last_ball_position is not None:\n",
    "        # Użyj ostatniej znanej pozycji piłki\n",
    "        x1, y1, x2, y2 = last_ball_position\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    else:\n",
    "        # Użyj pozycji zawodników\n",
    "        center_x, center_y = calculate_center(detections, width, height, use_ball=False)\n",
    "    \n",
    "    # Wygładzanie liniowe\n",
    "    center_x = prev_center_x * (1 - smoothing_factor) + center_x * smoothing_factor\n",
    "    center_y = prev_center_y * (1 - smoothing_factor) + center_y * smoothing_factor\n",
    "    prev_center_x, prev_center_y = center_x, center_y\n",
    "    \n",
    "    # Wygładzanie średnią kroczącą\n",
    "    positions_x.append(center_x)\n",
    "    positions_y.append(center_y)\n",
    "    if len(positions_x) > window_size:\n",
    "        positions_x.pop(0)\n",
    "        positions_y.pop(0)\n",
    "    smoothed_center_x = sum(positions_x) / len(positions_x)\n",
    "    smoothed_center_y = sum(positions_y) / len(positions_y)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    top = int(max(0, min(smoothed_center_y - crop_height / 2, height - crop_height)))\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    \n",
    "    # Przycięcie oryginalnego obrazu\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Narysowanie detekcji na przyciętym obrazie\n",
    "    draw_detections(cropped_frame, detections, left, top, right, bottom)\n",
    "    \n",
    "    # Skalowanie do 1280x720, jeśli kadr nie jest dokładny\n",
    "    if cropped_frame.shape != (crop_height, crop_width, 3):\n",
    "        cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Zapis klatki do tymczasowego pliku\n",
    "    out.write(cropped_frame)\n",
    "    \n",
    "    # Aktualizacja paska postępu\n",
    "    pbar.update(1)\n",
    "\n",
    "# Zamknięcie paska postępu\n",
    "pbar.close()\n",
    "\n",
    "# Zwolnienie zasobów OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dodanie dźwięku za pomocą moviepy\n",
    "print(\"Adding audio to the output video...\")\n",
    "input_clip = VideoFileClip(input_path)\n",
    "temp_clip = VideoFileClip(temp_video_path)\n",
    "\n",
    "# Ustawienie dźwięku z oryginalnego wideo\n",
    "final_clip = temp_clip.set_audio(input_clip.audio)\n",
    "\n",
    "# Zapis finalnego wideo z dźwiękiem\n",
    "final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "# Zamknięcie klipów\n",
    "input_clip.close()\n",
    "temp_clip.close()\n",
    "final_clip.close()\n",
    "\n",
    "# Usunięcie tymczasowego pliku\n",
    "os.remove(temp_video_path)\n",
    "print(\"Output video with audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 959/959 [00:25<00:00, 38.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding audio to the output video...\n",
      "Moviepy - Building video test.mp4.\n",
      "MoviePy - Writing audio in testTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n",
      "Output video with audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Wyłączenie szczegółowych logów YOLO\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "# Sprawdzenie dostępności GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "model = YOLO('yolov9c.pt').to(device)  # Przeniesienie modelu na GPU, jeśli dostępne\n",
    "\n",
    "# Mapowanie indeksów klas na nazwy (zamiast zmiany model.names)\n",
    "class_names = {0: \"player\", 1: \"referee\", 2: \"ball\"}\n",
    "\n",
    "# Ścieżki do plików\n",
    "input_path = '/media/512GB_ext/BurzaVSZryw_mini.mp4'\n",
    "output_path = 'test.mp4'\n",
    "temp_video_path = f'temp_output_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Docelowy rozmiar kadru (16:9, wysokość 720)\n",
    "crop_height = 720\n",
    "crop_width = int(crop_height * 16 / 9)  # 1280\n",
    "scale = 1  # Skala do zmniejszenia obrazu\n",
    "\n",
    "# Parametry wygładzania\n",
    "window_size = 15\n",
    "positions_x = []\n",
    "positions_y = []\n",
    "smoothing_factor = 0.1\n",
    "\n",
    "# Parametry dla skupienia na piłce\n",
    "frames_without_ball_threshold = int(fps * 3)  # 3 sekundy\n",
    "frames_without_ball = 0\n",
    "last_ball_position = None\n",
    "\n",
    "# Inicjalizacja zapisu tymczasowego wideo bez dźwięku\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(temp_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "def resize_frame(frame, scale=scale):\n",
    "    \"\"\"Zmniejsza obraz o podany współczynnik skali.\"\"\"\n",
    "    return cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def scale_detections(detections, scale):\n",
    "    \"\"\"Przeskalowuje współrzędne detekcji do oryginalnych wymiarów.\"\"\"\n",
    "    for det in detections:\n",
    "        det[0] /= scale  # x1\n",
    "        det[2] /= scale  # x2\n",
    "        det[1] /= scale  # y1\n",
    "        det[3] /= scale  # y2\n",
    "    return detections\n",
    "\n",
    "def draw_detections(frame, detections, left, top, right, bottom):\n",
    "    \"\"\"Rysuje bounding boxy i nazwy klas na przyciętym obrazie.\"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        # Sprawdzenie, czy detekcja jest w przyciętym obszarze\n",
    "        if x1 < right and x2 > left and y1 < bottom and y2 > top:\n",
    "            # Przesunięcie współrzędnych do przyciętego obszaru\n",
    "            draw_x1 = int(max(x1 - left, 0))\n",
    "            draw_y1 = int(max(y1 - top, 0))\n",
    "            draw_x2 = int(min(x2 - left, crop_width))\n",
    "            draw_y2 = int(min(y2 - top, crop_height))\n",
    "            label = class_names[int(cls)]  # Użycie mapowania klas\n",
    "            cv2.rectangle(frame, (draw_x1, draw_y1), (draw_x2, draw_y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (draw_x1, draw_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "def calculate_center(detections, frame_width, frame_height, use_ball=False):\n",
    "    \"\"\"Oblicza centrum akcji, priorytetyzując piłkę lub zawodników/sędziów.\"\"\"\n",
    "    if use_ball:\n",
    "        balls = [det for det in detections if int(det[5]) == 2]  # 'ball'\n",
    "        if balls:\n",
    "            x1, y1, x2, y2 = balls[0][:4]\n",
    "            return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    players_referees = [det for det in detections if int(det[5]) in [0, 1]]  # 'player' lub 'referee'\n",
    "    if players_referees:\n",
    "        centroids_x = [(det[0] + det[2]) / 2 for det in players_referees]\n",
    "        centroids_y = [(det[1] + det[3]) / 2 for det in players_referees]\n",
    "        return sum(centroids_x) / len(centroids_x), sum(centroids_y) / len(centroids_y)\n",
    "    return frame_width // 2, frame_height // 2\n",
    "\n",
    "prev_center_x, prev_center_y = width // 2, height // 2\n",
    "\n",
    "# Pasek postępu\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Zmniejszenie obrazu o połowę dla detekcji\n",
    "    small_frame = resize_frame(frame, scale=scale)\n",
    "    small_height, small_width = small_frame.shape[:2]\n",
    "    \n",
    "    # Detekcja na pomniejszonym obrazie\n",
    "    results = model(small_frame, classes=[0, 1, 2], device=device)  # Ogranicz do 'player', 'referee', 'ball'\n",
    "    detections = results[0].boxes.data.cpu().numpy()\n",
    "    \n",
    "    # Przeskalowanie detekcji do oryginalnych wymiarów\n",
    "    detections = scale_detections(detections, scale=scale)\n",
    "    \n",
    "    # Sprawdzenie, czy wykryto piłkę\n",
    "    balls = [det for det in detections if int(det[5]) == 2]\n",
    "    if balls:\n",
    "        frames_without_ball = 0\n",
    "        last_ball_position = balls[0][:4]\n",
    "    else:\n",
    "        frames_without_ball += 1\n",
    "    \n",
    "    # Decyzja, czy skupić się na piłce czy zawodnikach/sędziach\n",
    "    #if frames_without_ball <= frames_without_ball_threshold and last_ball_position is not None:\n",
    "        # Użyj ostatniej znanej pozycji piłki\n",
    "    #    x1, y1, x2, y2 = last_ball_position\n",
    "    #    center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    #else:\n",
    "        # Użyj pozycji zawodników/sędziów\n",
    "    center_x, center_y = calculate_center(detections, width, height, use_ball=False)\n",
    "    \n",
    "    # Wygładzanie liniowe\n",
    "    center_x = prev_center_x * (1 - smoothing_factor) + center_x * smoothing_factor\n",
    "    center_y = prev_center_y * (1 - smoothing_factor) + center_y * smoothing_factor\n",
    "    prev_center_x, prev_center_y = center_x, center_y\n",
    "    \n",
    "    # Wygładzanie średnią kroczącą\n",
    "    positions_x.append(center_x)\n",
    "    positions_y.append(center_y)\n",
    "    if len(positions_x) > window_size:\n",
    "        positions_x.pop(0)\n",
    "        positions_y.pop(0)\n",
    "    smoothed_center_x = sum(positions_x) / len(positions_x)\n",
    "    smoothed_center_y = sum(positions_y) / len(positions_y)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    top = int(max(0, min(smoothed_center_y - crop_height / 2, height - crop_height)))\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    \n",
    "    # Przycięcie oryginalnego obrazu\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Narysowanie detekcji na przyciętym obrazie\n",
    "    draw_detections(cropped_frame, detections, left, top, right, bottom)\n",
    "    \n",
    "    # Skalowanie do 1280x720, jeśli kadr nie jest dokładny\n",
    "    if cropped_frame.shape != (crop_height, crop_width, 3):\n",
    "        cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Zapis klatki do tymczasowego pliku\n",
    "    out.write(cropped_frame)\n",
    "    \n",
    "    # Aktualizacja paska postępu\n",
    "    pbar.update(1)\n",
    "\n",
    "# Zamknięcie paska postępu\n",
    "pbar.close()\n",
    "\n",
    "# Zwolnienie zasobów OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dodanie dźwięku za pomocą moviepy\n",
    "print(\"Adding audio to the output video...\")\n",
    "input_clip = VideoFileClip(input_path)\n",
    "temp_clip = VideoFileClip(temp_video_path)\n",
    "\n",
    "# Ustawienie dźwięku z oryginalnego wideo\n",
    "final_clip = temp_clip.set_audio(input_clip.audio)\n",
    "\n",
    "# Zapis finalnego wideo z dźwiękiem\n",
    "final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "# Zamknięcie klipów\n",
    "input_clip.close()\n",
    "temp_clip.close()\n",
    "final_clip.close()\n",
    "\n",
    "# Usunięcie tymczasowego pliku\n",
    "os.remove(temp_video_path)\n",
    "print(\"Output video with audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 1332/1332 [00:29<00:00, 44.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding audio to the output video...\n",
      "Moviepy - Building video test.mp4.\n",
      "MoviePy - Writing audio in testTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n",
      "Output video with audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Wyłączenie szczegółowych logów YOLO\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "# Sprawdzenie dostępności GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "model = YOLO('yolov9c.pt').to(device)  # Przeniesienie modelu na GPU, jeśli dostępne\n",
    "\n",
    "# Mapowanie indeksów klas na nazwy\n",
    "class_names = {0: \"player\", 1: \"referee\", 2: \"ball\"}\n",
    "\n",
    "# Ścieżki do plików\n",
    "input_path = 'input_01.mp4'\n",
    "output_path = 'test.mp4'\n",
    "temp_video_path = f'temp_output_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Docelowy rozmiar kadru (16:9, wysokość 720)\n",
    "crop_height = 720\n",
    "crop_width = int(crop_height * 16 / 9)  # 1280\n",
    "\n",
    "# Parametry wygładzania\n",
    "window_size = 15\n",
    "positions_x = []\n",
    "positions_y = []\n",
    "smoothing_factor = 0.1\n",
    "\n",
    "# Parametry dla skupienia na piłce\n",
    "frames_without_ball_threshold = int(fps * 3)  # 3 sekundy\n",
    "frames_without_ball = 0\n",
    "last_ball_position = None\n",
    "prev_ball_centroid = None\n",
    "movement_threshold = 10  # Piksele - próg ruchu piłki\n",
    "\n",
    "# Inicjalizacja zapisu tymczasowego wideo bez dźwięku\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(temp_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "def resize_frame(frame, scale=0.5):\n",
    "    \"\"\"Zmniejsza obraz o podany współczynnik skali.\"\"\"\n",
    "    return cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def scale_detections(detections, scale):\n",
    "    \"\"\"Przeskalowuje współrzędne detekcji do oryginalnych wymiarów.\"\"\"\n",
    "    for det in detections:\n",
    "        det[0] /= scale  # x1\n",
    "        det[2] /= scale  # x2\n",
    "        det[1] /= scale  # y1\n",
    "        det[3] /= scale  # y2\n",
    "    return detections\n",
    "\n",
    "def draw_detections(frame, detections, left, top, right, bottom):\n",
    "    \"\"\"Rysuje bounding boxy i nazwy klas na przyciętym obrazie dla wszystkich klas.\"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        if x1 < right and x2 > left and y1 < bottom and y2 > top:\n",
    "            draw_x1 = int(max(x1 - left, 0))\n",
    "            draw_y1 = int(max(y1 - top, 0))\n",
    "            draw_x2 = int(min(x2 - left, crop_width))\n",
    "            draw_y2 = int(min(y2 - top, crop_height))\n",
    "            label = class_names[int(cls)]  # Użycie mapowania klas\n",
    "            color = (0, 255, 0) if int(cls) == 2 else (255, 0, 0)  # Zielony dla piłki, niebieski dla graczy/sędziów\n",
    "            cv2.rectangle(frame, (draw_x1, draw_y1), (draw_x2, draw_y2), color, 2)\n",
    "            cv2.putText(frame, label, (draw_x1, draw_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "def calculate_center(detections, frame_width, frame_height, use_ball=True):\n",
    "    \"\"\"Oblicza centrum akcji, priorytetyzując piłkę lub zawodników/sędziów.\"\"\"\n",
    "    if use_ball:\n",
    "        balls = [det for det in detections if int(det[5]) == 2]  # 'ball'\n",
    "        if balls:\n",
    "            x1, y1, x2, y2 = balls[0][:4]\n",
    "            return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    players_referees = [det for det in detections if int(det[5]) in [0, 1]]  # 'player' lub 'referee'\n",
    "    if players_referees:\n",
    "        centroids_x = [(det[0] + det[2]) / 2 for det in players_referees]\n",
    "        centroids_y = [(det[1] + det[3]) / 2 for det in players_referees]\n",
    "        return sum(centroids_x) / len(centroids_x), sum(centroids_y) / len(centroids_y)\n",
    "    return frame_width // 2, frame_height // 2\n",
    "\n",
    "def is_moving_ball(ball_det, prev_centroid, threshold):\n",
    "    \"\"\"Sprawdza, czy piłka się porusza na podstawie przesunięcia centroidu.\"\"\"\n",
    "    if prev_centroid is None:\n",
    "        return True  # Pierwsza detekcja, uznajemy za ruch\n",
    "    x1, y1, x2, y2 = ball_det[:4]\n",
    "    current_centroid = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "    distance = np.linalg.norm(np.array(current_centroid) - np.array(prev_centroid))\n",
    "    return distance > threshold\n",
    "\n",
    "prev_center_x, prev_center_y = width // 2, height // 2\n",
    "\n",
    "# Pasek postępu\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Zmniejszenie obrazu o połowę dla detekcji\n",
    "    small_frame = resize_frame(frame, scale=0.5)\n",
    "    small_height, small_width = small_frame.shape[:2]\n",
    "    \n",
    "    # Detekcja na pomniejszonym obrazie\n",
    "    results = model(small_frame, classes=[0, 1, 2], device=device)  # Ogranicz do 'player', 'referee', 'ball'\n",
    "    detections = results[0].boxes.data.cpu().numpy()\n",
    "    \n",
    "    # Przeskalowanie detekcji do oryginalnych wymiarów\n",
    "    detections = scale_detections(detections, scale=0.5)\n",
    "    \n",
    "    # Filtrowanie detekcji piłek\n",
    "    balls = [det for det in detections if int(det[5]) == 2]\n",
    "    \n",
    "    # Sprawdzenie, czy jest piłka w ruchu\n",
    "    moving_balls = [ball for ball in balls if is_moving_ball(ball, prev_ball_centroid, movement_threshold)]\n",
    "    \n",
    "    if moving_balls:\n",
    "        # Użyj pierwszej piłki w ruchu\n",
    "        ball_det = moving_balls[0]\n",
    "        frames_without_ball = 0\n",
    "        last_ball_position = ball_det[:4]\n",
    "        x1, y1, x2, y2 = last_ball_position\n",
    "        prev_ball_centroid = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "    elif balls:\n",
    "        # Użyj statycznej piłki, jeśli nie ma piłki w ruchu\n",
    "        ball_det = balls[0]\n",
    "        last_ball_position = ball_det[:4]\n",
    "        x1, y1, x2, y2 = last_ball_position\n",
    "        prev_ball_centroid = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "        frames_without_ball += 1\n",
    "    else:\n",
    "        frames_without_ball += 1\n",
    "        prev_ball_centroid = None  # Resetuj, jeśli nie ma detekcji\n",
    "    \n",
    "    # Decyzja, czy skupić się na piłce czy zawodnikach/sędziach\n",
    "    if frames_without_ball <= frames_without_ball_threshold and last_ball_position is not None:\n",
    "        # Użyj ostatniej znanej pozycji piłki\n",
    "        x1, y1, x2, y2 = last_ball_position\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    else:\n",
    "        # Użyj pozycji zawodników/sędziów\n",
    "        center_x, center_y = calculate_center(detections, width, height, use_ball=False)\n",
    "    \n",
    "    # Wygładzanie liniowe\n",
    "    center_x = prev_center_x * (1 - smoothing_factor) + center_x * smoothing_factor\n",
    "    center_y = prev_center_y * (1 - smoothing_factor) + center_y * smoothing_factor\n",
    "    prev_center_x, prev_center_y = center_x, center_y\n",
    "    \n",
    "    # Wygładzanie średnią kroczącą\n",
    "    positions_x.append(center_x)\n",
    "    positions_y.append(center_y)\n",
    "    if len(positions_x) > window_size:\n",
    "        positions_x.pop(0)\n",
    "        positions_y.pop(0)\n",
    "    smoothed_center_x = sum(positions_x) / len(positions_x)\n",
    "    smoothed_center_y = sum(positions_y) / len(positions_y)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    top = int(max(0, min(smoothed_center_y - crop_height / 2, height - crop_height)))\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    \n",
    "    # Przycięcie oryginalnego obrazu\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Narysowanie detekcji na przyciętym obrazie\n",
    "    draw_detections(cropped_frame, detections, left, top, right, bottom)\n",
    "    \n",
    "    # Skalowanie do 1280x720, jeśli kadr nie jest dokładny\n",
    "    if cropped_frame.shape != (crop_height, crop_width, 3):\n",
    "        cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Zapis klatki do tymczasowego pliku\n",
    "    out.write(cropped_frame)\n",
    "    \n",
    "    # Aktualizacja paska postępu\n",
    "    pbar.update(1)\n",
    "\n",
    "# Zamknięcie paska postępu\n",
    "pbar.close()\n",
    "\n",
    "# Zwolnienie zasobów OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dodanie dźwięku za pomocą moviepy\n",
    "print(\"Adding audio to the output video...\")\n",
    "input_clip = VideoFileClip(input_path)\n",
    "temp_clip = VideoFileClip(temp_video_path)\n",
    "\n",
    "# Ustawienie dźwięku z oryginalnego wideo\n",
    "final_clip = temp_clip.set_audio(input_clip.audio)\n",
    "\n",
    "# Zapis finalnego wideo z dźwiękiem\n",
    "final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "# Zamknięcie klipów\n",
    "input_clip.close()\n",
    "temp_clip.close()\n",
    "final_clip.close()\n",
    "\n",
    "# Usunięcie tymczasowego pliku\n",
    "os.remove(temp_video_path)\n",
    "print(\"Output video with audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kadrowanie od lewej do prawej - tylko zawodnicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 959/959 [00:28<00:00, 33.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding audio to the output video...\n",
      "Moviepy - Building video test.mp4.\n",
      "MoviePy - Writing audio in testTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n",
      "Output video with audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Wyłączenie szczegółowych logów YOLO\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "# Sprawdzenie dostępności GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Wczytanie modelu YOLO\n",
    "model = YOLO('yolov9c.pt').to(device)\n",
    "\n",
    "# Mapowanie indeksów klas na nazwy\n",
    "class_names = {0: \"player\", 1: \"referee\", 2: \"ball\"}\n",
    "\n",
    "# Ścieżki do plików\n",
    "input_path = '/media/512GB_ext/BurzaVSZryw_mini.mp4'\n",
    "output_path = 'test.mp4'\n",
    "temp_video_path = f'temp_output_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4'\n",
    "\n",
    "# Otwarcie wideo\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Docelowy rozmiar kadru (16:9, wysokość 720)\n",
    "crop_height = height\n",
    "crop_width = int(crop_height * 16 / 9)  # 1280\n",
    "\n",
    "# Parametry wygładzania\n",
    "window_size = 15\n",
    "positions_x = []\n",
    "smoothing_factor = 0.1\n",
    "\n",
    "# Definicja stref: lewa 40%, środkowa 20%, prawa 40%\n",
    "left_zone = 0.4 * width\n",
    "middle_zone_start = 0.4 * width\n",
    "middle_zone_end = 0.6 * width\n",
    "right_zone = 0.6 * width\n",
    "\n",
    "# Inicjalizacja zapisu tymczasowego wideo bez dźwięku\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(temp_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "def resize_frame(frame, scale=0.5):\n",
    "    \"\"\"Zmniejsza obraz o podany współczynnik skali.\"\"\"\n",
    "    return cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def scale_detections(detections, scale):\n",
    "    \"\"\"Przeskalowuje współrzędne detekcji do oryginalnych wymiarów.\"\"\"\n",
    "    for det in detections:\n",
    "        det[0] /= scale  # x1\n",
    "        det[2] /= scale  # x2\n",
    "        det[1] /= scale  # y1\n",
    "        det[3] /= scale  # y2\n",
    "    return detections\n",
    "\n",
    "def draw_detections(frame, detections, left, top, right, bottom):\n",
    "    \"\"\"Rysuje bounding boxy i nazwy klas na przyciętym obrazie dla wszystkich klas.\"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        if x1 < right and x2 > left and y1 < bottom and y2 > top:\n",
    "            draw_x1 = int(max(x1 - left, 0))\n",
    "            draw_y1 = int(max(y1 - top, 0))\n",
    "            draw_x2 = int(min(x2 - left, crop_width))\n",
    "            draw_y2 = int(min(y2 - top, crop_height))\n",
    "            label = class_names[int(cls)]\n",
    "            color = (0, 255, 0) if int(cls) == 2 else (255, 0, 0)\n",
    "            cv2.rectangle(frame, (draw_x1, draw_y1), (draw_x2, draw_y2), color, 2)\n",
    "            cv2.putText(frame, label, (draw_x1, draw_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "def calculate_player_median(detections):\n",
    "    \"\"\"Oblicza medianę położenia zawodników w poziomie.\"\"\"\n",
    "    players = [det for det in detections if int(det[5]) == 0]  # 'player'\n",
    "    if players:\n",
    "        centroids_x = [(det[0] + det[2]) / 2 for det in players]\n",
    "        return np.median(centroids_x)\n",
    "    return None\n",
    "\n",
    "prev_center_x = width // 2\n",
    "\n",
    "# Pasek postępu\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Zmniejszenie obrazu o połowę dla detekcji\n",
    "    small_frame = resize_frame(frame, scale=0.5)\n",
    "    small_height, small_width = small_frame.shape[:2]\n",
    "    \n",
    "    # Detekcja na pomniejszonym obrazie\n",
    "    results = model(small_frame, classes=[0, 1, 2], device=device)\n",
    "    detections = results[0].boxes.data.cpu().numpy()\n",
    "    \n",
    "    # Przeskalowanie detekcji do oryginalnych wymiarów\n",
    "    detections = scale_detections(detections, scale=0.5)\n",
    "    \n",
    "    # Obliczenie mediany położenia zawodników\n",
    "    median_x = calculate_player_median(detections)\n",
    "    \n",
    "    if median_x is not None:\n",
    "        # Określenie docelowego centrum na podstawie stref\n",
    "        if median_x < left_zone:\n",
    "            target_center_x = crop_width / 2  # Lewa granica\n",
    "        elif median_x > right_zone:\n",
    "            target_center_x = width - crop_width / 2  # Prawa granica\n",
    "        else:\n",
    "            target_center_x = width / 2  # Środek\n",
    "    else:\n",
    "        target_center_x = width / 2\n",
    "    \n",
    "    # Wygładzanie liniowe\n",
    "    center_x = prev_center_x * (1 - smoothing_factor) + target_center_x * smoothing_factor\n",
    "    prev_center_x = center_x\n",
    "    \n",
    "    # Wygładzanie średnią kroczącą\n",
    "    positions_x.append(center_x)\n",
    "    if len(positions_x) > window_size:\n",
    "        positions_x.pop(0)\n",
    "    smoothed_center_x = sum(positions_x) / len(positions_x)\n",
    "    \n",
    "    # Obliczenie pozycji kadru\n",
    "    left = int(max(0, min(smoothed_center_x - crop_width / 2, width - crop_width)))\n",
    "    right = left + crop_width\n",
    "    top = 0  # Kadr stały w pionie\n",
    "    bottom = height\n",
    "    \n",
    "    # Przycięcie oryginalnego obrazu\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Narysowanie detekcji na przyciętym obrazie\n",
    "    draw_detections(cropped_frame, detections, left, top, right, bottom)\n",
    "    \n",
    "    # Skalowanie do 1280x720, jeśli kadr nie jest dokładny\n",
    "    if cropped_frame.shape != (crop_height, crop_width, 3):\n",
    "        cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Zapis klatki do tymczasowego pliku\n",
    "    out.write(cropped_frame)\n",
    "    \n",
    "    # Aktualizacja paska postępu\n",
    "    pbar.update(1)\n",
    "\n",
    "# Zamknięcie paska postępu\n",
    "pbar.close()\n",
    "\n",
    "# Zwolnienie zasobów OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dodanie dźwięku za pomocą moviepy\n",
    "print(\"Adding audio to the output video...\")\n",
    "input_clip = VideoFileClip(input_path)\n",
    "temp_clip = VideoFileClip(temp_video_path)\n",
    "\n",
    "# Ustawienie dźwięku z oryginalnego wideo\n",
    "final_clip = temp_clip.set_audio(input_clip.audio)\n",
    "\n",
    "# Zapis finalnego wideo z dźwiękiem\n",
    "final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "\n",
    "# Zamknięcie klipów\n",
    "input_clip.close()\n",
    "temp_clip.close()\n",
    "final_clip.close()\n",
    "\n",
    "# Usunięcie tymczasowego pliku\n",
    "os.remove(temp_video_path)\n",
    "print(\"Output video with audio saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
